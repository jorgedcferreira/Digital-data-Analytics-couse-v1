{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M51nz-RSAY3Q"
   },
   "source": [
    "# R&D "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZBy_NnwnAY3R"
   },
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KDZYDxt7AY3S"
   },
   "source": [
    "*pip install pyspark*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rKdzTFaIAY3S"
   },
   "source": [
    "## Imports section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 367,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 852,
     "status": "error",
     "timestamp": 1521626707894,
     "user": {
      "displayName": "Rui Machado",
      "photoUrl": "//lh5.googleusercontent.com/-lkZceWn3OlE/AAAAAAAAAAI/AAAAAAAAC0c/jTLw7Bj4-qo/s50-c-k-no/photo.jpg",
      "userId": "102045935416745437263"
     },
     "user_tz": 0
    },
    "id": "G6iL6qx5AY3T",
    "outputId": "8ce8a337-345d-432e-8cdf-b7b6e623d77d"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession # Novo no PySpark 2.0\n",
    "from pyspark.sql.types import StructType\\\n",
    "                            , StructField\\\n",
    "                            , IntegerType\\\n",
    "                            , StringType\\\n",
    "                            , DoubleType\\\n",
    "                            , DateType\\\n",
    "                            , TimestampType\\\n",
    "                            , BooleanType # Avaliar e inferir schemas\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7WZK0G7AY3W"
   },
   "source": [
    "## Ler dados com PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "TuiupJDAAY3X"
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate() # especifico do spark 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iR_QFni8AY3Y"
   },
   "source": [
    "*Ler de forma schemeless*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {}
     ]
    },
    "colab_type": "code",
    "id": "hADw8NvCAY3Z",
    "outputId": "d63e4628-dd66-47f3-fdc6-ed990cb28cbf"
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "#df=spark.read.csv('./datasets/loan_data.csv',header=True,inferSchema=True)\n",
    "# inferSchema não é bom para grandes volumes de dados\n",
    "# é preferivel fazer manualmente como demonstrado em baixo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ezuGmrHRAY3d"
   },
   "source": [
    "*Ler com schema definido - Faster*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {}
     ]
    },
    "colab_type": "code",
    "id": "bh0pUalhAY3d",
    "outputId": "403ccaca-5d88-47f3-a883-6931da1c1b9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 20 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# NomeCampo, TipoDados, Nullable\n",
    "df_schema=StructType([\n",
    "    StructField('ip', IntegerType(),True),\n",
    "    StructField('app', IntegerType(),True),\n",
    "    StructField('device', IntegerType(),True),\n",
    "    StructField('os', IntegerType(),True),\n",
    "    StructField('channel', IntegerType(),True),\n",
    "    StructField('click_time', DateType(),True),\n",
    "    StructField('attributed_time', DateType(),True),\n",
    "    StructField('is_attributed', IntegerType(),True)\n",
    "])\n",
    "\n",
    "# ler os dados mas com schema definido - Improvements de velocidade muito grandes\n",
    "df=spark.read.csv('./datasets/train_sample.csv',header=True,schema=df_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QAdO5a1TAY3g"
   },
   "source": [
    "## Cleaning data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "plhV4j1QAY3h"
   },
   "source": [
    "*Existem linhas duplicadas?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ip: int, app: int, device: int, os: int, channel: int, click_time: timestamp, attributed_time: timestamp, is_attributed: int]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {}
     ]
    },
    "colab_type": "code",
    "id": "WmSSizUkAY3i",
    "outputId": "7882ad7a-12ee-42e2-8c09-f51fbb23d84a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ip: string, app: int, device: int, os: int, channel: int, click_time: timestamp, attributed_time: timestamp, is_attributed: int]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.withColumn(\"ip\", df[\"ip\"].cast(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o27.collectToPython.\n: java.lang.IllegalArgumentException\r\n\tat org.apache.xbean.asm5.ClassReader.<init>(Unknown Source)\r\n\tat org.apache.xbean.asm5.ClassReader.<init>(Unknown Source)\r\n\tat org.apache.xbean.asm5.ClassReader.<init>(Unknown Source)\r\n\tat org.apache.spark.util.ClosureCleaner$.getClassReader(ClosureCleaner.scala:46)\r\n\tat org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:449)\r\n\tat org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:432)\r\n\tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\r\n\tat scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)\r\n\tat scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)\r\n\tat scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)\r\n\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)\r\n\tat scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103)\r\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\r\n\tat org.apache.spark.util.FieldAccessFinder$$anon$3.visitMethodInsn(ClosureCleaner.scala:432)\r\n\tat org.apache.xbean.asm5.ClassReader.a(Unknown Source)\r\n\tat org.apache.xbean.asm5.ClassReader.b(Unknown Source)\r\n\tat org.apache.xbean.asm5.ClassReader.accept(Unknown Source)\r\n\tat org.apache.xbean.asm5.ClassReader.accept(Unknown Source)\r\n\tat org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:262)\r\n\tat org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:261)\r\n\tat scala.collection.immutable.List.foreach(List.scala:381)\r\n\tat org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:261)\r\n\tat org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:159)\r\n\tat org.apache.spark.SparkContext.clean(SparkContext.scala:2292)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2066)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2092)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:297)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply$mcI$sp(Dataset.scala:3195)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3192)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3192)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\r\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:3225)\r\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3192)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.base/java.lang.Thread.run(Thread.java:844)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-d26192a85e47>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#.groupBy('attributed_time').count().show()\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\rpmachado\\venvs\\dev-env-2\\lib\\site-packages\\pyspark\\sql\\dataframe.pyc\u001b[0m in \u001b[0;36mtoPandas\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1964\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s\\n%s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_exception_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1965\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1966\u001b[1;33m             \u001b[0mpdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1967\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1968\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rpmachado\\venvs\\dev-env-2\\lib\\site-packages\\pyspark\\sql\\dataframe.pyc\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    464\u001b[0m         \"\"\"\n\u001b[0;32m    465\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m             \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    467\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    468\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rpmachado\\venvs\\dev-env-2\\lib\\site-packages\\py4j\\java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1160\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1162\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rpmachado\\venvs\\dev-env-2\\lib\\site-packages\\pyspark\\sql\\utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rpmachado\\venvs\\dev-env-2\\lib\\site-packages\\py4j\\protocol.pyc\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    318\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    319\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    321\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o27.collectToPython.\n: java.lang.IllegalArgumentException\r\n\tat org.apache.xbean.asm5.ClassReader.<init>(Unknown Source)\r\n\tat org.apache.xbean.asm5.ClassReader.<init>(Unknown Source)\r\n\tat org.apache.xbean.asm5.ClassReader.<init>(Unknown Source)\r\n\tat org.apache.spark.util.ClosureCleaner$.getClassReader(ClosureCleaner.scala:46)\r\n\tat org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:449)\r\n\tat org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:432)\r\n\tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\r\n\tat scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)\r\n\tat scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)\r\n\tat scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)\r\n\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)\r\n\tat scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103)\r\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\r\n\tat org.apache.spark.util.FieldAccessFinder$$anon$3.visitMethodInsn(ClosureCleaner.scala:432)\r\n\tat org.apache.xbean.asm5.ClassReader.a(Unknown Source)\r\n\tat org.apache.xbean.asm5.ClassReader.b(Unknown Source)\r\n\tat org.apache.xbean.asm5.ClassReader.accept(Unknown Source)\r\n\tat org.apache.xbean.asm5.ClassReader.accept(Unknown Source)\r\n\tat org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:262)\r\n\tat org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:261)\r\n\tat scala.collection.immutable.List.foreach(List.scala:381)\r\n\tat org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:261)\r\n\tat org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:159)\r\n\tat org.apache.spark.SparkContext.clean(SparkContext.scala:2292)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2066)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2092)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:297)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply$mcI$sp(Dataset.scala:3195)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3192)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3192)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\r\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:3225)\r\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3192)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.base/java.lang.Thread.run(Thread.java:844)\r\n"
     ]
    }
   ],
   "source": [
    "df.toPandas()#.groupBy('attributed_time').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+------+---+-------+-------------------+---------------+-------------+\n",
      "|    ip|app|device| os|channel|         click_time|attributed_time|is_attributed|\n",
      "+------+---+------+---+-------+-------------------+---------------+-------------+\n",
      "| 87540| 12|     1| 13|    497|2017-11-07 09:30:38|           null|            0|\n",
      "|105560| 25|     1| 17|    259|2017-11-07 13:40:27|           null|            0|\n",
      "|101424| 12|     1| 19|    212|2017-11-07 18:05:24|           null|            0|\n",
      "| 94584| 13|     1| 13|    477|2017-11-07 04:58:08|           null|            0|\n",
      "| 68413| 12|     1|  1|    178|2017-11-09 09:00:09|           null|            0|\n",
      "| 93663|  3|     1| 17|    115|2017-11-09 01:22:13|           null|            0|\n",
      "| 17059|  1|     1| 17|    135|2017-11-09 01:17:58|           null|            0|\n",
      "|121505|  9|     1| 25|    442|2017-11-07 10:01:53|           null|            0|\n",
      "|192967|  2|     2| 22|    364|2017-11-08 09:35:17|           null|            0|\n",
      "|143636|  3|     1| 19|    135|2017-11-08 12:35:26|           null|            0|\n",
      "| 73839|  3|     1| 22|    489|2017-11-08 08:14:37|           null|            0|\n",
      "| 34812|  3|     1| 13|    489|2017-11-07 05:03:14|           null|            0|\n",
      "|114809|  3|     1| 22|    205|2017-11-09 10:24:23|           null|            0|\n",
      "|114220|  6|     1| 20|    125|2017-11-08 14:46:16|           null|            0|\n",
      "| 36150|  2|     1| 13|    205|2017-11-07 00:54:09|           null|            0|\n",
      "| 72116| 25|     2| 19|    259|2017-11-08 23:17:45|           null|            0|\n",
      "|  5314|  2|     1|  2|    477|2017-11-09 07:33:41|           null|            0|\n",
      "|106598|  3|     1| 20|    280|2017-11-09 03:44:35|           null|            0|\n",
      "| 72065| 20|     2| 90|    259|2017-11-06 23:14:08|           null|            0|\n",
      "| 37301| 14|     1| 13|    349|2017-11-06 20:07:00|           null|            0|\n",
      "+------+---+------+---+-------+-------------------+---------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|    attributed_time|\n",
      "+-------------------+\n",
      "|2017-11-07 09:30:38|\n",
      "|2017-11-07 13:40:27|\n",
      "|2017-11-07 18:05:24|\n",
      "|2017-11-07 04:58:08|\n",
      "|2017-11-09 09:00:09|\n",
      "|2017-11-09 01:22:13|\n",
      "|2017-11-09 01:17:58|\n",
      "|2017-11-07 10:01:53|\n",
      "|2017-11-08 09:35:17|\n",
      "|2017-11-08 12:35:26|\n",
      "|2017-11-08 08:14:37|\n",
      "|2017-11-07 05:03:14|\n",
      "|2017-11-09 10:24:23|\n",
      "|2017-11-08 14:46:16|\n",
      "|2017-11-07 00:54:09|\n",
      "|2017-11-08 23:17:45|\n",
      "|2017-11-09 07:33:41|\n",
      "|2017-11-09 03:44:35|\n",
      "|2017-11-06 23:14:08|\n",
      "|2017-11-06 20:07:00|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"attributed_time\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BaA1R-lIAY3l"
   },
   "source": [
    "*Eliminar linhas duplicadas*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "bvw2j1TUAY3l"
   },
   "outputs": [],
   "source": [
    "# Eliminar duplicados na combinação de todas as linhas\n",
    "df = df.dropDuplicates()\n",
    "\n",
    "# Eliminar duplicados para uma linha especifica\n",
    "df = df.dropDuplicates(subset=[\n",
    "    c for c in df.columns if c != 'id'\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SNAN4_zYAY3n"
   },
   "source": [
    "## Playing with data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iv9OyjOcAY3o"
   },
   "source": [
    "*Quantos propositos distintos de crédito existem?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "fPIVBZe9AY3o"
   },
   "outputs": [],
   "source": [
    "df.select('purpose').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bvt3SzbXAY3q"
   },
   "source": [
    "*Quantos emprestimos foram realizados agrupados por propositos crédito?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "4sZTNTQoAY3r"
   },
   "outputs": [],
   "source": [
    "df.select('purpose').groupBy('purpose').count().orderBy('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B0e2xo_KAY3s"
   },
   "source": [
    "*Quantos emprestimos foram realizados agrupados por propositos crédito em gráfico?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ank8_I1PAY3t"
   },
   "outputs": [],
   "source": [
    "res=df.select('purpose').groupBy('purpose').count().orderBy('count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "iKppVI4PAY3w"
   },
   "outputs": [],
   "source": [
    "display(res)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "dev.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "jupyter2_Python_2",
   "language": "python",
   "name": "jupyter2_python_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
